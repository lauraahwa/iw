{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22002,"status":"ok","timestamp":1736373538051,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"},"user_tz":-480},"id":"W6GCUrU3sTiN","outputId":"e763efaa-c25e-4acd-c542-102784bdd40f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10407,"status":"ok","timestamp":1736373552171,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"},"user_tz":-480},"id":"HzS7KgjWsODz"},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","import spacy\n","from spacy.lang.zh.examples import sentences\n","\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","import numpy as np\n","\n","import torch\n","from torch.nn.utils.rnn import pad_sequence # not from scratch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split, Subset\n","import random\n","import os\n","\n","from tqdm import tqdm\n","\n","\n","import matplotlib.pyplot as plt\n","from scipy.stats import mode"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1736387927655,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"},"user_tz":-480},"id":"wquhm8GDsnr_"},"outputs":[],"source":["# RULE 1: Subject Verb Agreement\n","def check_subject_verb_agreement(tagged_tokens):\n","    \"\"\"\n","    Checks for subject-verb agreement errors in a sentence.\n","    This function assumes that the input is a list of (word, POS) tuples.\n","\n","    Rules:\n","    - Singular subjects (e.g., he, she, it, singular nouns) must pair with third-person singular verbs (VBZ).\n","    - Plural subjects (e.g., they, plural nouns) must pair with plural verb forms (VBP).\n","    - First-person singular (I) and second-person singular/plural (you) have specific verb forms.\n","\n","    Parameters:\n","        tagged_tokens (list of tuples): List of (word, POS) tuples.\n","\n","    Returns:\n","        int: Number of subject-verb agreement errors detected.\n","    \"\"\"\n","    errors = 0\n","    singular_pronouns = {\"he\", \"she\", \"it\"}  # Singular pronouns\n","    plural_pronouns = {\"we\", \"they\"}         # Plural pronouns\n","    second_person_pronouns = {\"you\"}         # Second person (singular/plural)\n","    first_person_singular = {\"i\"}            # First person singular\n","    verb_dict = {\"VB\", \"VBP\", \"VBZ\", \"VBD\", \"VBG\", \"VBN\"}\n","\n","    subject_found = None  # Keep track of the last subject found\n","\n","    for i, (word, pos) in enumerate(tagged_tokens):\n","        # Identify subjects (pronouns or nouns)\n","        if pos == \"PRP\" or pos == \"NN\" or pos == \"NNS\":\n","            subject_found = (word.lower(), pos)\n","\n","        # Check verbs only if a subject has been found\n","        if subject_found and pos in verb_dict:\n","            subj_word, subj_pos = subject_found\n","\n","            # Handle singular pronouns\n","            if subj_pos == \"PRP\" and subj_word in singular_pronouns:\n","                if pos != \"VBZ\":\n","                    errors += 1\n","\n","            # Handle plural pronouns\n","            elif subj_pos == \"PRP\" and subj_word in plural_pronouns:\n","                if pos != \"VBP\":\n","                    errors += 1\n","\n","            # Handle second-person pronouns\n","            elif subj_pos == \"PRP\" and subj_word in second_person_pronouns:\n","                if pos not in {\"VB\", \"VBP\"}:\n","                    errors += 1\n","\n","            # Handle first-person singular pronouns\n","            elif subj_pos == \"PRP\" and subj_word in first_person_singular:\n","                if pos != \"VBP\":\n","                    errors += 1\n","\n","            # Handle singular nouns\n","            elif subj_pos == \"NN\":\n","                if pos != \"VBZ\":\n","                    errors += 1\n","\n","            # Handle plural nouns\n","            elif subj_pos == \"NNS\":\n","                if pos != \"VBP\":\n","                    errors += 1\n","\n","            # Reset subject_found for compound predicates\n","            subject_found = None\n","\n","    return errors"]},{"cell_type":"code","source":["# helper function to enforce RULE 2\n","# not all-encompassing\n","word_to_num = {\n","    \"one\": 1,\n","    \"two\": 2,\n","    \"three\": 3,\n","    \"four\": 4,\n","    \"five\": 5,\n","    \"six\": 6,\n","    \"seven\": 7,\n","    \"eight\": 8,\n","    \"nine\": 9,\n","    \"ten\": 10,\n","    \"hundred\": 100,\n","    \"hundreds\": 100,\n","    \"thousand\": 1000,\n","    \"thousands\": 1000,\n","    \"million\": 10e6,\n","    \"millions\": 10e6\n","}\n","\n","def word_to_number(word):\n","    \"\"\"\n","    Converts a word or digit string into an integer.\n","\n","    Parameters:\n","        word (str): The input word or string.\n","\n","    Returns:\n","        int: The corresponding integer value, or None if not valid.\n","    \"\"\"\n","    if word.isdigit():  # Check if it's a numeric string\n","        return int(word)\n","    elif word.lower() in word_to_num:  # Check if it's a number word\n","        return word_to_num[word.lower()]\n","    return None"],"metadata":{"id":"3_Ro7RGkVY1V","executionInfo":{"status":"ok","timestamp":1736373562060,"user_tz":-480,"elapsed":373,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# RULE 2: Singular/Plural Noun Confusion\n","def check_singular_plural_confusion(tagged_tokens):\n","    \"\"\"\n","    Checks for singular/plural noun confusion errors based on POS tags and context.\n","    Uses Penn Treebank POS tags to detect errors in singular/plural noun usage.\n","\n","    Parameters:\n","        tagged_tokens (list of tuples): List of (word, POS) tuples.\n","\n","    Returns:\n","        int: Number of singular/plural noun confusion errors detected.\n","    \"\"\"\n","    errors = 0\n","    for i, (word, pos) in enumerate(tagged_tokens):\n","        number = word_to_number(word)\n","        # Check for singular determiners or numbers\n","        if pos in {\"DT\", \"CD\"} and (word.lower() in {\"a\", \"an\", \"one\", \"each\", \"every\", \"this\", \"that\"} or (number == 1)):\n","            # Look for the next noun\n","            for j in range(i + 1, len(tagged_tokens)):\n","                if tagged_tokens[j][1] in {\"NN\", \"NNS\"}:\n","                    if tagged_tokens[j][1] == \"NNS\":  # Plural noun found where singular expected\n","                        errors += 1\n","                    break\n","\n","        # Check for plural determiners or numbers\n","        elif pos in {\"DT\", \"CD\", \"JJ\", \"PDT\"} and (word.lower() in {\"many\", \"several\", \"these\", \"those\", \"all\", \"both\"} or (number and number > 1)):\n","            # Look for the next noun\n","            for j in range(i + 1, len(tagged_tokens)):\n","                if tagged_tokens[j][1] in {\"NN\", \"NNS\"}:\n","                    if tagged_tokens[j][1] == \"NN\":  # Singular noun found where plural expected\n","                        errors += 1\n","                    break\n","\n","    return errors"],"metadata":{"id":"d4tklTxBIiN9","executionInfo":{"status":"ok","timestamp":1736373565068,"user_tz":-480,"elapsed":279,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# RULE 3: Verb Tense Confusion\n","def check_verb_tense_confusion(tagged_tokens):\n","    \"\"\"\n","    Checks for verb tense confusion errors based on temporal expressions and verb tenses.\n","    Uses Penn Treebank POS tags to detect mismatches between temporal context and verb forms.\n","\n","    Parameters:\n","        tagged_tokens (list of tuples): List of (word, POS) tuples.\n","\n","    Returns:\n","        int: Number of verb tense confusion errors detected.\n","    \"\"\"\n","    errors = 0\n","\n","    # Temporal keywords categorized by tense\n","    past_markers = {\"yesterday\", \"last\", \"ago\", \"earlier\"}\n","    present_markers = {\"today\", \"now\", \"currently\"}\n","    future_markers = {\"tomorrow\", \"next\", \"later\", \"soon\", \"will\"}\n","    verb_dict = {\"VB\", \"VBD\", \"VBZ\", \"VBP\", \"MD\", \"VBG\", \"VBN\"}\n","\n","    skip_next = False # skip checking next token if it's part of an auxiliary phrase\n","\n","    for i, (word, pos) in enumerate(tagged_tokens):\n","        if skip_next:\n","          skip_next = False\n","          continue\n","\n","         # Check for temporal markers\n","        if word.lower() in past_markers:\n","            # Look for the nearest verb after the temporal marker\n","            for j in range(i + 1, len(tagged_tokens)):\n","                if tagged_tokens[j][1] in {\"VB\", \"VBD\", \"VBZ\", \"VBP\", \"MD\", \"VBG\", \"VBN\"}:\n","                    if tagged_tokens[j][1] in {\"VBD\", \"VBN\"}:  # Past tense is correct\n","                        if tagged_tokens[j][1] == \"VBD\" or tagged_tokens[j - 1][1] in {\"was\", \"had\"}:\n","                            skip_next = True  # Skip compound tense verbs\n","                        break\n","                    else:\n","                        errors += 1  # Past tense mismatch\n","                    break\n","\n","        elif word.lower() in present_markers:\n","            # Look for the nearest verb after the temporal marker\n","            for j in range(i + 1, len(tagged_tokens)):\n","                if tagged_tokens[j][1] in {\"VB\", \"VBD\", \"VBZ\", \"VBP\", \"MD\", \"VBG\", \"VBN\"}:\n","                    if tagged_tokens[j][1] in {\"VBZ\", \"VBP\", \"VBG\"}:  # Present tense is correct\n","                        if tagged_tokens[j][1] == \"VBG\" or tagged_tokens[j - 1][1] in {\"is\", \"has\"}:\n","                            skip_next = True  # Skip compound tense verbs\n","                        break\n","                    else:\n","                        errors += 1  # Present tense mismatch\n","                    break\n","\n","        elif word.lower() in future_markers:\n","            # Look for the nearest verb after the temporal marker\n","            for j in range(i + 1, len(tagged_tokens)):\n","                if tagged_tokens[j][1] in {\"VB\", \"VBD\", \"VBZ\", \"VBP\", \"MD\", \"VBG\", \"VBN\"}:\n","                    if tagged_tokens[j][1] == \"MD\":  # Future tense auxiliary found\n","                        skip_next = True  # Skip the next token (part of auxiliary/modal phrase)\n","                        break\n","                    elif tagged_tokens[j][1] != \"MD\":  # Future tense mismatch\n","                        errors += 1\n","                    break\n","    return errors"],"metadata":{"id":"6ZdQx5Ah_tPL","executionInfo":{"status":"ok","timestamp":1736373567775,"user_tz":-480,"elapsed":282,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# RULE 4: Omitting/Inserting Articles\n","def check_articles(tagged_tokens):\n","  \"\"\"\n","  Checks for errors related to omitting or inserting articles in sentences.\n","  Uses Penn Treebank POS tags to detect errors in article usage.\n","\n","  Parameters:\n","      tagged_tokens (list of tuples): List of (word, POS) tuples.\n","\n","  Returns:\n","      int: Number of article-related errors detected.\n","  \"\"\"\n","  errors = 0\n","  articles = {\"a\", \"an\", \"the\"}\n","  # uncountable nouns are not comprehensive but common ones are included\n","  uncountable_nouns = {\"homework\", \"air\", \"furniture\", \"information\", \"advice\",\n","        \"rice\", \"fear\", \"safety\", \"water\", \"beauty\", \"knowledge\", \"love\",\n","        \"research\", \"advice\", \"work\", \"bread\", \"traffic\", \"travel\", \"weather\", \"news\",\n","        \"soccer\", \"tennis\", \"basketball\", \"swimming\", \"baseball\", \"dinner\"}\n","  # Add more as needed\n","\n","  for i, (word, pos) in enumerate(tagged_tokens):\n","      # Check for missing articles before singular countable nouns (NN)\n","      if pos == \"NN\":\n","          if i > 0 and tagged_tokens[i - 1][1] == \"DT\":  # If an article exists\n","              # Check for redundant articles with uncountable nouns\n","              if word.lower() in uncountable_nouns and tagged_tokens[i - 1][0].lower() in {\"a\", \"an\"}:\n","                  errors += 1  # Redundant article for uncountable noun\n","              continue\n","          elif word.lower() not in uncountable_nouns:  # Singular countable nouns need articles\n","              # Check if the noun is preceded by a non-article determiner (e.g., \"my\", \"this\")\n","              if i == 0 or tagged_tokens[i - 1][1] not in {\"DT\", \"PRP$\", \"JJ\", \"CD\"}:\n","                  errors += 1  # Missing article\n","\n","      # Check for redundant articles before uncountable nouns or plural nouns (NNS)\n","      if pos == \"NNS\" and i > 0 and tagged_tokens[i - 1][1] == \"DT\":\n","          prev_word = tagged_tokens[i - 1][0].lower()\n","          if prev_word in {\"a\", \"an\"}:  # Indefinite articles are invalid before plural nouns\n","              errors += 1  # Redundant or incorrect article\n","      # Check for conjunctions before proceeding to the next word\n","      if pos in {\"NNS\", \"NN\"} and i > 0 and tagged_tokens[i - 1][1] == \"CC\":\n","          continue  # Skip validation for conjunctions like \"and\"\n","\n","      # Check for improper articles with proper nouns (NNP, NNPS)\n","      if pos in {\"NNP\", \"NNPS\"} and i > 0 and tagged_tokens[i - 1][1] == \"DT\":\n","          if word.lower() not in {\"earth\", \"moon\", \"sun\"}:  # Exceptions where articles are valid\n","              errors += 1  # Redundant article\n","\n","  return errors"],"metadata":{"id":"0bcwhVW0YSIc","executionInfo":{"status":"ok","timestamp":1736388840955,"user_tz":-480,"elapsed":284,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1270,"status":"ok","timestamp":1736373581529,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"},"user_tz":-480},"id":"xMCICxtPsuL-"},"outputs":[],"source":["nlp_en = spacy.load(\"en_core_web_sm\")\n","\n","def get_ptb_tags(sentence):\n","  doc = nlp_en(sentence)\n","  return [(token.text, token.tag_) for token in doc]"]},{"cell_type":"code","source":["def test_rule_fct(rules_fct, sentences):\n","  results = {}\n","  for sentence in sentences:\n","    tagged_tokens = get_ptb_tags(sentence)\n","    # print(f\"Sentence: {sentence}\")\n","    # print(f\"Tagged Tokens: {tagged_tokens}\")\n","\n","    errors = rules_fct(tagged_tokens)\n","    results[sentence] = errors\n","    # print(f\"{rules_fct.__name__} Errors detected: {errors}\\n\")\n","\n","  return results"],"metadata":{"id":"3Mx2E__ZT0qG","executionInfo":{"status":"ok","timestamp":1736393012048,"user_tz":-480,"elapsed":370,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1736388838576,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"},"user_tz":-480},"id":"ICrkC5Uft40c"},"outputs":[],"source":["sentences_1 = [\n","  \"He like cheese.\",\n","  \"The dog bark loudly.\",\n","  \"I am really happy that you were able to attend our function this evening.\",\n","  \"The bark of the tree is rough.\",\n","  \"I am making dinner in the kitchen. I am cooking the chicken and vegetables.\"\n","  ]"]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1736391004485,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"},"user_tz":-480},"id":"3rViJXLmvHGF","outputId":"8253f077-154f-4ce8-f650-6fd58802655a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: He like cheese.\n","Tagged Tokens: [('He', 'PRP'), ('like', 'VBP'), ('cheese', 'NN'), ('.', '.')]\n","check_subject_verb_agreement Errors detected: 1\n","\n","Sentence: The dog bark loudly.\n","Tagged Tokens: [('The', 'DT'), ('dog', 'NN'), ('bark', 'NN'), ('loudly', 'RB'), ('.', '.')]\n","check_subject_verb_agreement Errors detected: 0\n","\n","Sentence: I am really happy that you were able to attend our function this evening.\n","Tagged Tokens: [('I', 'PRP'), ('am', 'VBP'), ('really', 'RB'), ('happy', 'JJ'), ('that', 'IN'), ('you', 'PRP'), ('were', 'VBD'), ('able', 'JJ'), ('to', 'TO'), ('attend', 'VB'), ('our', 'PRP$'), ('function', 'NN'), ('this', 'DT'), ('evening', 'NN'), ('.', '.')]\n","check_subject_verb_agreement Errors detected: 1\n","\n","Sentence: The bark of the tree is rough.\n","Tagged Tokens: [('The', 'DT'), ('bark', 'NN'), ('of', 'IN'), ('the', 'DT'), ('tree', 'NN'), ('is', 'VBZ'), ('rough', 'JJ'), ('.', '.')]\n","check_subject_verb_agreement Errors detected: 0\n","\n","Sentence: I am making dinner in the kitchen. I am cooking the chicken and vegetables.\n","Tagged Tokens: [('I', 'PRP'), ('am', 'VBP'), ('making', 'VBG'), ('dinner', 'NN'), ('in', 'IN'), ('the', 'DT'), ('kitchen', 'NN'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('cooking', 'VBG'), ('the', 'DT'), ('chicken', 'NN'), ('and', 'CC'), ('vegetables', 'NNS'), ('.', '.')]\n","check_subject_verb_agreement Errors detected: 0\n","\n"]}],"source":["results = test_rule_fct(check_subject_verb_agreement, sentences_1)"]},{"cell_type":"code","source":["print(type(results))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eItxF8gH7rOY","executionInfo":{"status":"ok","timestamp":1736390396235,"user_tz":-480,"elapsed":5,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}},"outputId":"0b4d3be9-05be-466a-f856-a97e8ed74b46"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'dict'>\n"]}]},{"cell_type":"code","execution_count":30,"metadata":{"id":"F02xlu3lye_A","executionInfo":{"status":"ok","timestamp":1736374527027,"user_tz":-480,"elapsed":283,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"outputs":[],"source":["sentences_2 = [\n","    \"We have three dog.\",\n","    \"One cats is on the roof.\",\n","    \"Many cat are sleeping.\",\n","    \"These tree are tall.\",\n","    \"Each dogs is friendly.\"\n","]"]},{"cell_type":"code","source":["results_2 = test_rule_fct(check_singular_plural_confusion, sentences_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72HkQZDGKF4t","executionInfo":{"status":"ok","timestamp":1736374528987,"user_tz":-480,"elapsed":268,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}},"outputId":"686f7ee4-de9f-49ed-d1af-0e4d95cd7f3f"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: We have three dog.\n","Tagged Tokens: [('We', 'PRP'), ('have', 'VBP'), ('three', 'CD'), ('dog', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: One cats is on the roof.\n","Tagged Tokens: [('One', 'CD'), ('cats', 'NNS'), ('is', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('roof', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: Many cat are sleeping.\n","Tagged Tokens: [('Many', 'JJ'), ('cat', 'NN'), ('are', 'VBP'), ('sleeping', 'VBG'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: These tree are tall.\n","Tagged Tokens: [('These', 'DT'), ('tree', 'NN'), ('are', 'VBP'), ('tall', 'JJ'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: Each dogs is friendly.\n","Tagged Tokens: [('Each', 'DT'), ('dogs', 'NNS'), ('is', 'VBZ'), ('friendly', 'JJ'), ('.', '.')]\n","Errors detected: 1\n","\n"]}]},{"cell_type":"code","source":["sentences_3 = [\n","    \"Yesterday she is running to the park.\",     # incorrect 1\n","    \"Today he ran to the store.\", # incorrect   1\n","    \"Tomorrow she shall run a marathon.\",# correct   0\n","    \"Last week he runs every day.\",     # incorrect 1\n","    \"Next year they build a new house.\",  # incorrect 1\n","    \"Now she is writing a letter.\", # correct 0\n","    \"Now she was writing a letter.\" # incorrect 1\n","]"],"metadata":{"id":"YWcI_jEJTIyt","executionInfo":{"status":"ok","timestamp":1736374533528,"user_tz":-480,"elapsed":632,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["results_3 = test_rule_fct(check_verb_tense_confusion, sentences_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJzSLsUQTsL2","executionInfo":{"status":"ok","timestamp":1736374539670,"user_tz":-480,"elapsed":236,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}},"outputId":"2a90dd72-dc24-4ef6-9ec8-087ce6b61529"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: Yesterday she is running to the park.\n","Tagged Tokens: [('Yesterday', 'NN'), ('she', 'PRP'), ('is', 'VBZ'), ('running', 'VBG'), ('to', 'IN'), ('the', 'DT'), ('park', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: Today he ran to the store.\n","Tagged Tokens: [('Today', 'NN'), ('he', 'PRP'), ('ran', 'VBD'), ('to', 'IN'), ('the', 'DT'), ('store', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: Tomorrow she shall run a marathon.\n","Tagged Tokens: [('Tomorrow', 'NN'), ('she', 'PRP'), ('shall', 'MD'), ('run', 'VB'), ('a', 'DT'), ('marathon', 'NN'), ('.', '.')]\n","Errors detected: 0\n","\n","Sentence: Last week he runs every day.\n","Tagged Tokens: [('Last', 'JJ'), ('week', 'NN'), ('he', 'PRP'), ('runs', 'VBZ'), ('every', 'DT'), ('day', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: Next year they build a new house.\n","Tagged Tokens: [('Next', 'JJ'), ('year', 'NN'), ('they', 'PRP'), ('build', 'VBP'), ('a', 'DT'), ('new', 'JJ'), ('house', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n","Sentence: Now she is writing a letter.\n","Tagged Tokens: [('Now', 'RB'), ('she', 'PRP'), ('is', 'VBZ'), ('writing', 'VBG'), ('a', 'DT'), ('letter', 'NN'), ('.', '.')]\n","Errors detected: 0\n","\n","Sentence: Now she was writing a letter.\n","Tagged Tokens: [('Now', 'RB'), ('she', 'PRP'), ('was', 'VBD'), ('writing', 'VBG'), ('a', 'DT'), ('letter', 'NN'), ('.', '.')]\n","Errors detected: 1\n","\n"]}]},{"cell_type":"code","source":["sentences_4 = [\n","    \"I went to store.\",                       # Error: Missing \"the\".\n","    \"He likes movie.\",                        # Error: Missing \"a\".\n","    \"The God blessed America.\",               # Error: Redundant \"The\".\n","    \"You gained the weight last month.\",      # Error: Redundant \"the\".\n","    \"She has a homework to finish.\",          # Error: Redundant \"a\".\n","    \"The Earth orbits the Sun.\",              # Correct (exception case for proper nouns).\n","    \"I want an information from you.\",        # Error: Uncountable noun\n","    \"I like to play soccer with my friends on the weekends.\", # Correct\n","    \"I am making dinner in the kitchen. I am cooking chicken and vegetables.\"\n","]"],"metadata":{"id":"_j2YB5YITvfU","executionInfo":{"status":"ok","timestamp":1736390871023,"user_tz":-480,"elapsed":250,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["results_4 = test_rule_fct(check_articles, sentences_4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WvJ-iTzh-DH","executionInfo":{"status":"ok","timestamp":1736390894641,"user_tz":-480,"elapsed":236,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}},"outputId":"dae56afd-46ab-49bf-db50-ccf6663d1b8a"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: I went to store.\n","Tagged Tokens: [('I', 'PRP'), ('went', 'VBD'), ('to', 'IN'), ('store', 'NN'), ('.', '.')]\n","check_articles Errors detected: 1\n","Sentence: He likes movie.\n","Tagged Tokens: [('He', 'PRP'), ('likes', 'VBZ'), ('movie', 'NN'), ('.', '.')]\n","check_articles Errors detected: 1\n","Sentence: The God blessed America.\n","Tagged Tokens: [('The', 'DT'), ('God', 'NNP'), ('blessed', 'VBD'), ('America', 'NNP'), ('.', '.')]\n","check_articles Errors detected: 1\n","Sentence: You gained the weight last month.\n","Tagged Tokens: [('You', 'PRP'), ('gained', 'VBD'), ('the', 'DT'), ('weight', 'NN'), ('last', 'JJ'), ('month', 'NN'), ('.', '.')]\n","check_articles Errors detected: 0\n","Sentence: She has a homework to finish.\n","Tagged Tokens: [('She', 'PRP'), ('has', 'VBZ'), ('a', 'DT'), ('homework', 'NN'), ('to', 'TO'), ('finish', 'VB'), ('.', '.')]\n","check_articles Errors detected: 1\n","Sentence: The Earth orbits the Sun.\n","Tagged Tokens: [('The', 'DT'), ('Earth', 'NNP'), ('orbits', 'VBZ'), ('the', 'DT'), ('Sun', 'NNP'), ('.', '.')]\n","check_articles Errors detected: 0\n","Sentence: I want an information from you.\n","Tagged Tokens: [('I', 'PRP'), ('want', 'VBP'), ('an', 'DT'), ('information', 'NN'), ('from', 'IN'), ('you', 'PRP'), ('.', '.')]\n","check_articles Errors detected: 1\n","Sentence: I like to play soccer with my friends on the weekends.\n","Tagged Tokens: [('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('play', 'VB'), ('soccer', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('friends', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('weekends', 'NNS'), ('.', '.')]\n","check_articles Errors detected: 0\n","Sentence: I am making dinner in the kitchen. I am cooking chicken and vegetables.\n","Tagged Tokens: [('I', 'PRP'), ('am', 'VBP'), ('making', 'VBG'), ('dinner', 'NN'), ('in', 'IN'), ('the', 'DT'), ('kitchen', 'NN'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('cooking', 'VBG'), ('chicken', 'NN'), ('and', 'CC'), ('vegetables', 'NNS'), ('.', '.')]\n","check_articles Errors detected: 1\n"]}]},{"cell_type":"code","source":["global_voices_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/global_voices_en_sentences.csv')\n","tle_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/tle_sentences.csv')\n","mac_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/mac_en_sentences.csv')\n","\n","# SYNTHETIC DATASET\n","sse_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/sse_sentences.csv')\n","sle_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/sle_sentences.csv')\n","\n","sse_sentences_df = sse_sentences_df.drop(columns=[\"Sentence\"])\n","sse_sentences_df = sse_sentences_df.rename(columns={\"Sentence.1\": \"Sentence\"})\n","sle_sentences_df = sle_sentences_df.rename(columns={\"0\": \"Sentence\"})"],"metadata":{"id":"sAwFi2hTue6q","executionInfo":{"status":"ok","timestamp":1736373649439,"user_tz":-480,"elapsed":1897,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["sse_sentences = sse_sentences_df['Sentence'].tolist()\n","sse_test = sse_sentences[:10]\n","sse_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POPcgUI0ukQc","executionInfo":{"status":"ok","timestamp":1736373652946,"user_tz":-480,"elapsed":264,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}},"outputId":"e7eb97b7-e5dc-42da-b133-191d71c7ad9b"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I am eating a sandwich for lunch today.',\n"," 'I went to the mall with my mom and bought a new toy.',\n"," 'My friends and I like to hang out at the park on weekends.',\n"," 'I eat my sandwich and fruit for lunch every day',\n"," 'I love spending time with my friends at the park.',\n"," 'I play soccer with my friends every Saturday.',\n"," 'I am making dinner in the kitchen. I am cooking chicken and vegetables.',\n"," 'I like to play soccer with my friends on the weekends.',\n"," 'I like hanging out with my friends at the park on weekends.',\n"," 'I went to the mall with my family and bought a new toy.']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["rule_functions = [\n","    check_subject_verb_agreement,\n","    check_singular_plural_confusion,\n","    check_verb_tense_confusion,\n","    check_articles,\n","]\n","\n","def cumulate(results):\n","    \"\"\"\n","    Sums the errors across all rules for each sentence.\n","\n","    Parameters:\n","        results (dict): A dictionary where keys are sentences and values are\n","                       dictionaries of rule names mapping to error counts.\n","\n","    Returns:\n","        dict: A dictionary where each sentence maps to its total error count.\n","    \"\"\"\n","    cumulative_errors = {}\n","    for sentence, rule_errors in results.items():\n","        cumulative_errors[sentence] = sum(rule_errors.values())\n","    return cumulative_errors\n","\n","def check_all_rules(sentence):\n","    \"\"\"\n","    Checks all rules for a list of sentences and sums error counts in parallel.\n","\n","    Parameters:\n","        sentences (list of str): List of sentences to be checked.\n","\n","    Returns:\n","        dict: A dictionary where each sentence maps to its total error count.\n","    \"\"\"\n","    # Pass all sentences and all rule functions to test_rule_fct\n","    all_results = test_rule_fct(rule_functions, sentences)\n","\n","    # Sum errors across all rules for each sentence\n","    cumulative_results = cumulate(all_results)\n","\n","    return cumulative_results"],"metadata":{"id":"WmRYMCc28Fsn","executionInfo":{"status":"ok","timestamp":1736390806096,"user_tz":-480,"elapsed":286,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["nlp_en = spacy.load(\"en_core_web_sm\")\n","\n","def get_ptb_tags(sentence):\n","  doc = nlp_en(sentence)\n","  return [(token.text, token.tag_) for token in doc]"],"metadata":{"id":"xBBJnHVP8MPn","executionInfo":{"status":"ok","timestamp":1736373757028,"user_tz":-480,"elapsed":1673,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def check_sentences(sentences):\n","    \"\"\"\n","    Checks multiple sentences for grammar errors using all rules.\n","\n","    Parameters:\n","        sentences (list of str): List of sentences to be checked.\n","\n","    Returns:\n","        dict: A dictionary where each sentence maps to its total error count.\n","    \"\"\"\n","    print(f\"Processing sentences: {sentences}\")\n","    total_errors = check_all_rules(sentences)\n","    return total_errors"],"metadata":{"id":"Lb7dhWwg8On4","executionInfo":{"status":"ok","timestamp":1736390811025,"user_tz":-480,"elapsed":290,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["def sum_errors_for_sentence(sentence, dict_collection):\n","  total_errors = 0\n","  for item in dict_collection:\n","    if sentence in item:\n","      total_errors += item[sentence]\n","  return total_errors"],"metadata":{"id":"13okyqYb_o_b","executionInfo":{"status":"ok","timestamp":1736391904561,"user_tz":-480,"elapsed":259,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["def get_error_rate(sentences):\n","\n","  dict_collection = []\n","\n","  for rule in rule_functions:\n","    dict_collection.append(test_rule_fct(rule, sentences))\n","\n","  total_errors = 0\n","  total_words = 0\n","\n","  for sentence in sentences:\n","    errors_for_sentence = sum_errors_for_sentence(sentence, dict_collection)\n","    total_errors += errors_for_sentence\n","\n","    # Count the number of words in the sentence\n","    num_words = len(sentence.split())\n","    total_words += num_words\n","\n","  average_error_rate = total_errors / total_words if total_words > 0 else 0\n","\n","  print(f\"\\nTotal Errors Across All Sentences: {total_errors}\")\n","  print(f\"Total Words Across All Sentences: {total_words}\")\n","  print(f\"Average Error Rate (errors per word): {average_error_rate:.4f}\")"],"metadata":{"id":"VkcusjZh-GoM","executionInfo":{"status":"ok","timestamp":1736393070522,"user_tz":-480,"elapsed":261,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["# GET ALL SENTENCES\n","\n","global_voices_en_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/global_voices_en_sentences.csv')\n","mac_en_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/mac_en_sentences.csv')\n","tle_sentences_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/tle_sentences.csv')\n","\n","sse_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/sse_sentences.csv')\n","sle_df = pd.read_csv('/content/drive/My Drive/IW_Codebase/rule_based_flow/sle_sentences.csv')\n","\n","sse_df = sse_df.drop(columns=[\"Sentence\"])\n","sse_df = sse_df.rename(columns={\"Sentence.1\": \"Sentence\"})\n","sle_df = sle_df.rename(columns={\"0\": \"Sentence\"})"],"metadata":{"id":"8YgkHOJ3CLRI","executionInfo":{"status":"ok","timestamp":1736393503734,"user_tz":-480,"elapsed":260,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":123,"outputs":[]},{"cell_type":"code","source":["global_voices_en_sentences = global_voices_en_df['sentence'].tolist()\n","mac_en_sentences = mac_en_sentences_df['sentence'].tolist()\n","tle_sentences = tle_sentences_df['sentence'].tolist()\n","\n","sse_sentences = sse_df['Sentence'].tolist()\n","sle_sentences = sle_df['Sentence'].tolist()\n","# sle_sentences"],"metadata":{"id":"aeSLH7oqHcid","executionInfo":{"status":"ok","timestamp":1736393761314,"user_tz":-480,"elapsed":343,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}}},"execution_count":137,"outputs":[]},{"cell_type":"code","source":["sentence_list = [global_voices_en_sentences, mac_en_sentences, tle_sentences, sse_sentences, sle_sentences]\n","\n","for item in sentence_list:\n","  get_error_rate(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHLXLrZ6H3BV","executionInfo":{"status":"ok","timestamp":1736394253174,"user_tz":-480,"elapsed":416368,"user":{"displayName":"Laura Hwa","userId":"17150300420817395494"}},"outputId":"b877317a-8ee3-4083-f7e9-4cd598375481"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Total Errors Across All Sentences: 5147\n","Total Words Across All Sentences: 35235\n","Average Error Rate (errors per word): 0.1461\n","\n","Total Errors Across All Sentences: 5229\n","Total Words Across All Sentences: 32314\n","Average Error Rate (errors per word): 0.1618\n","\n","Total Errors Across All Sentences: 3813\n","Total Words Across All Sentences: 28327\n","Average Error Rate (errors per word): 0.1346\n","\n","Total Errors Across All Sentences: 2864\n","Total Words Across All Sentences: 23492\n","Average Error Rate (errors per word): 0.1219\n","\n","Total Errors Across All Sentences: 4319\n","Total Words Across All Sentences: 19119\n","Average Error Rate (errors per word): 0.2259\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UTYIy1pIInTv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPz4TIxnv2sIvaZvFD+GEeX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}